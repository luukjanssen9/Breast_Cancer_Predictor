{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bfb4c8-4207-4b49-a914-087063992a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Downloading kagglehub-0.3.10-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8bea2cf4-fd49-4ab4-b167-2cd95283d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/sofiafischel/.cache/kagglehub/datasets/utkarshx27/breast-cancer-wisconsin-diagnostic-dataset/versions/1\n",
      "Files in dataset directory: ['brca.csv']\n"
     ]
    }
   ],
   "source": [
    "#Loading in dataset from Kaggle\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"utkarshx27/breast-cancer-wisconsin-diagnostic-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Files in dataset directory:\", os.listdir(path))\n",
    "\n",
    "csv_file = os.path.join(path, \"brca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2271bf10-be28-4921-89f1-ee88b89ef4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix)\n",
    "from sklearn.metrics import (mean_squared_error, accuracy_score, precision_score, recall_score)\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "y = df['y']\n",
    "df.drop('y', axis='columns', inplace=True)\n",
    "\n",
    "#MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "model=scaler.fit(df)\n",
    "scaled_data=model.transform(df)\n",
    "scaled_data=pd.DataFrame(data = scaled_data, columns = df.columns)\n",
    "df = scaled_data.join(y)\n",
    "\n",
    "#Using all 30 features\n",
    "X = df.drop('y', axis='columns')\n",
    "y = df['y']\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state = 42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predict = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7a222fdb-8261-4598-b3e6-2b28818210ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "[1.         0.96491228 0.98245614 0.98245614 1.         1.\n",
      " 0.98245614 1.         1.         0.98214286]\n",
      "Average Accuracy =  0.9894423558897243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "CV = cross_validate(lr, X, y, cv=10, scoring=[\"accuracy\"])\n",
    "print(\"Accuracy\")\n",
    "print(CV[\"test_accuracy\"])\n",
    "\n",
    "print(\"Average Accuracy = \", sum(CV[\"test_accuracy\"]) / len(CV[\"test_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "fb4fe051-8f6d-4535-9803-7f6bb0145124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability value for chosen class: 0.9421112821637193 \n",
      "\n",
      "Average probability value for class B: 0.9493748281028732\n",
      "Average probability value without outliers for class B: 0.9689073073929567 \n",
      "\n",
      "Average probability value for class M: 0.9296594891251698\n",
      "Average probability value without outliers for class M: 0.9532050978191594 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Probability Analysis\n",
    "from scipy import stats\n",
    "probabilities = lr.predict_proba(X_test)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "avgMaxVals = np.mean(np.max(probabilities, axis=1))\n",
    "print(f\"Average probability value for chosen class: {avgMaxVals} \\n\")\n",
    "\n",
    "#Average probability score of all cells classed as B\n",
    "probabilitiesB = probabilities[:, 0]\n",
    "filteredProbabilitiesB = probabilitiesB[predict == 'B']\n",
    "avgBVals = np.mean(filteredProbabilitiesB)\n",
    "print(f\"Average probability value for class B: {avgBVals}\")\n",
    "#Excluding outliers\n",
    "z = np.abs(stats.zscore(filteredProbabilitiesB))\n",
    "inliersIndices = np.where(z < 2)[0]\n",
    "filteredProbabilitiesB_NoOutliers = filteredProbabilitiesB[inliersIndices]\n",
    "print(f\"Average probability value without outliers for class B: {np.mean(filteredProbabilitiesB_NoOutliers)} \\n\")\n",
    "\n",
    "#Average probability score of all cells classed as M\n",
    "probabilitiesM = probabilities[:, 1]\n",
    "filteredProbabilitiesM = probabilitiesM[predict == 'M']\n",
    "avgMVals = np.mean(filteredProbabilitiesM)\n",
    "print(f\"Average probability value for class M: {avgMVals}\")\n",
    "#Excluding outliers\n",
    "z = np.abs(stats.zscore(filteredProbabilitiesM))\n",
    "inliersIndices = np.where(z < 2)[0]\n",
    "filteredProbabilitiesM_NoOutliers = filteredProbabilitiesM[inliersIndices]\n",
    "print(f\"Average probability value without outliers for class M: {np.mean(filteredProbabilitiesM_NoOutliers)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f6042290-8068-44ad-ba84-3b4829ac5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.99      1.00      0.99        71\n",
      "           M       1.00      0.98      0.99        43\n",
      "\n",
      "    accuracy                           0.99       114\n",
      "   macro avg       0.99      0.99      0.99       114\n",
      "weighted avg       0.99      0.99      0.99       114\n",
      "\n",
      "Accuracy score: 0.9912280701754386 \n",
      "\n",
      "True Positives (TP): 42\n",
      "False Positives (FP): 0\n",
      "True Negatives (TN): 71\n",
      "False Negatives (FN): 1 \n",
      "\n",
      "Recall - TP/(TP+FN): 0.9767441860465116\n",
      "False Negative Rate - FN/(TP+FN): 0.023255813953488372\n"
     ]
    }
   ],
   "source": [
    "#Classic metrics analysis\n",
    "print(f\"Classification report: \\n {classification_report(y_test, predict)}\")\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, predict)} \\n\")\n",
    "\n",
    "matrix = confusion_matrix(y_test, predict)\n",
    "TN, FP, FN, TP = matrix.ravel()\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Negatives (FN): {FN} \\n\")\n",
    "\n",
    "print(f\"Recall - TP/(TP+FN): {TP/(TP+FN)}\")\n",
    "print(f\"False Negative Rate - FN/(TP+FN): {FN/(TP+FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "9ec0d4ec-1f44-476f-90c7-bf64f5468a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "\n",
      "\n",
      "Confusion Matrix for 0:\n",
      "  True Positives (TP): 72\n",
      "  False Positives (FP): 0\n",
      "  True Negatives (TN): 42\n",
      "  False Negatives (FN): 0\n",
      "\n",
      "\n",
      "Confusion Matrix for 1:\n",
      "  True Positives (TP): 42\n",
      "  False Positives (FP): 0\n",
      "  True Negatives (TN): 72\n",
      "  False Negatives (FN): 0\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        72\n",
      "           M       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           1.00       114\n",
      "   macro avg       1.00      1.00      1.00       114\n",
      "weighted avg       1.00      1.00      1.00       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix for both classes (essentially overkill!)\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, multilabel_confusion_matrix)\n",
    "from sklearn.metrics import (mean_squared_error, accuracy_score, precision_score, recall_score)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predict))\n",
    "\n",
    "allMatricies = multilabel_confusion_matrix(y_test, predict)\n",
    "print(\"\\n\")\n",
    "for i, matrix in enumerate(allMatricies):\n",
    "    TN, FP, FN, TP = matrix.ravel()\n",
    "\n",
    "    #Explicitly writing out the confusion matrix values for each category\n",
    "    print(f\"Confusion Matrix for {i}:\")\n",
    "    print(f\"  True Positives (TP): {TP}\")\n",
    "    print(f\"  False Positives (FP): {FP}\")\n",
    "    print(f\"  True Negatives (TN): {TN}\")\n",
    "    print(f\"  False Negatives (FN): {FN}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(classification_report(y_test, predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ba460980-c763-4a9c-9dc2-ba9fda4b4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "[0.92982456 0.85964912 0.96491228 0.94736842 0.94736842 0.98245614\n",
      " 0.92982456 1.         0.94736842 0.94642857]\n",
      "Average Accuracy =  0.9455200501253133\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.91      0.99      0.95        71\n",
      "           M       0.97      0.84      0.90        43\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.94      0.91      0.92       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n",
      "Accuracy score: 0.9298245614035088 \n",
      "\n",
      "True Positives (TP): 36\n",
      "False Positives (FP): 1\n",
      "True Negatives (TN): 70\n",
      "False Negatives (FN): 7 \n",
      "\n",
      "Recall - TP/(TP+FN): 0.8372093023255814\n",
      "False Negative Rate - FN/(TP+FN): 0.16279069767441862\n"
     ]
    }
   ],
   "source": [
    "#Using the top five most important features\n",
    "df = pd.read_csv(csv_file)\n",
    "y = df['y']\n",
    "df.drop('y', axis='columns', inplace=True)\n",
    "\n",
    "#MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "model=scaler.fit(df)\n",
    "scaled_data=model.transform(df)\n",
    "scaled_data=pd.DataFrame(data = scaled_data, columns = df.columns)\n",
    "df = scaled_data.join(y)\n",
    "\n",
    "#Using top 5 features\n",
    "X = df[['x.area_worst', 'x.concave_pts_worst', 'x.radius_worst', 'x.perimeter_worst', 'x.concave_pts_mean']]\n",
    "y = df['y']\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state = 42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predict = lr.predict(X_test)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "CV = cross_validate(lr, X, y, cv=10, scoring=[\"accuracy\"])\n",
    "print(\"Accuracy\")\n",
    "print(CV[\"test_accuracy\"])\n",
    "\n",
    "print(\"Average Accuracy = \", sum(CV[\"test_accuracy\"]) / len(CV[\"test_accuracy\"]))\n",
    "\n",
    "#Classic metrics analysis\n",
    "print(f\"Classification report: \\n {classification_report(y_test, predict)}\")\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, predict)} \\n\")\n",
    "\n",
    "matrix = confusion_matrix(y_test, predict)\n",
    "TN, FP, FN, TP = matrix.ravel()\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Negatives (FN): {FN} \\n\")\n",
    "\n",
    "print(f\"Recall - TP/(TP+FN): {TP/(TP+FN)}\")\n",
    "print(f\"False Negative Rate - FN/(TP+FN): {FN/(TP+FN)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
